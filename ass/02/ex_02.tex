\ExTitle{2}

\begin{aufgabe}
\end{aufgabe}
\begin{enumerate}[a)]
	\item Satz von Bayes: Für zwei Ereignisse A und B (B$\not=$0): P(A$|$B) steht im Verhältnis zu P(B$|$A):\\
	$P(A|B)=\frac{P(A|B)\cdot P(A)}{P(B)}$\\
	Beispiele aus der Vorlesung:\\
	
	\item evidence = Prob(Daten), diese ist 1, wenn sie unabhängig von der Hypothese ist und kann in diesem Fall im Bruch vernachlässigt werden. Sollte die Hypothese nicht unabhängig sein, ist die Zahl unterm Bruch nicht 1 und kann nicht vernachlässigt werden.\\
	
	\item Man entscheidet sich für die größere A-Priori Wahrscheinlichkeit.\\
	
	\item Auf Seite 36 wird die A-Priori Wahrscheinlichkeit betrachtet, auf Seite 38 die A-Posteriori Wahrscheinlichkeit. In Seite 38 summieren sich die Funktionswerte punktweise zu 1, da die Wahrscheinlichkeit für den Funktionswert x für Hypothese 1 genau die Gegenwahrscheinlichkeit zum Funktionswert x unter Hypothese 2 ist.\\
	Auf Seite 36 ist noch keine Aussage über die Wahrscheinlichkeit der HYpothese unter Betrachtung der Daten möglich.\\
	Beide Funktionen auf beiden Seiten haben das Integral 1, da es sich um Wahrscheinlichkeitsdichtefunktionen handelt.\\
	
	\item Sie besagt, dass man die Wahrscheinlichkeit für Fehler minimieren soll.\\
	$\omega_1 $, wenn P$(\omega_1|x)> $P$(\omega_2|x)$, sonst $\omega_2$.\\
	
	\item Das conditional risk ist der Erwartungswert des Verlustes der mit einer Entscheidung verbunden ist und drückt sich aus durch:\\
	$R(\alpha_i|x)=\sum_j\lambda(\alpha_i|\omega_j)P(\omega_j|x)$\\
	
	\item Bayes risk ist das kleinste Gesamtrisiko, das erreicht werden kann.\\
	
	
	\end{enumerate}